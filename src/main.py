# RAG Demo - åŸºäºå‘˜å·¥æ‰‹å†Œçš„é—®ç­”ç³»ç»Ÿ

import os
import sys
from dotenv import load_dotenv

# ç¦ç”¨ ChromaDB é¥æµ‹åŠŸèƒ½ä»¥é¿å…é”™è¯¯
os.environ["ANONYMIZED_TELEMETRY"] = "False"

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from loaders.pdf_loader import PDFLoader
from splitters import HeadingBasedSplitter
from embeddings import OllamaEmbeddings
from vectorstore import ChromaVectorStore
from chains.qa_chain import BasicQAChain

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class EmployeeHandbookQA:
    """å‘˜å·¥æ‰‹å†Œé—®ç­”ç³»ç»Ÿ"""
    
    def __init__(
        self,
        pdf_path: str = "data/å‘˜å·¥æ‰‹å†Œ.pdf",
        collection_name: str = "employee_handbook",
        persist_directory: str = "./chroma_db",
        max_chunk_size: int = 1500,
        chunk_overlap: int = 100,
        embedding_model: str = "BAAI/bge-m3",
        llm_model: str = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
    ):
        """
        åˆå§‹åŒ–å‘˜å·¥æ‰‹å†Œé—®ç­”ç³»ç»Ÿ
        
        Args:
            pdf_path: å‘˜å·¥æ‰‹å†ŒPDFæ–‡ä»¶è·¯å¾„
            collection_name: å‘é‡æ•°æ®åº“é›†åˆåç§°
            persist_directory: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
            max_chunk_size: æ–‡æœ¬å—æœ€å¤§å­—ç¬¦æ•°ï¼ˆç”¨äºè¿‡é•¿çš„æ ‡é¢˜å—ï¼‰
            chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            llm_model: LLMæ¨¡å‹åç§°
        """
        self.pdf_path = pdf_path
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.max_chunk_size = max_chunk_size
        self.chunk_overlap = chunk_overlap
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embedder = None
        self.store = None
        self.qa_chain = None
        
        print("=" * 60)
        print("ğŸš€ å‘˜å·¥æ‰‹å†Œé—®ç­”ç³»ç»Ÿ")
        print("=" * 60)
    
    def initialize(self, force_rebuild: bool = False):
        """
        åˆå§‹åŒ–ç³»ç»Ÿ
        
        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“
        """
        print("\nğŸ“‹ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print(f"\nğŸ”¤ åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model}")
        self.embedder = OllamaEmbeddings(model=self.embedding_model)
        
        # 2. åˆå§‹åŒ–å‘é‡å­˜å‚¨
        print(f"\nğŸ’¾ åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {self.collection_name}")
        self.store = ChromaVectorStore(
            collection_name=self.collection_name,
            persist_directory=self.persist_directory
        )
        self.store.create_collection(self.embedder.embed_query)
        
        # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºå‘é‡æ•°æ®åº“
        collection_info = self.store.get_collection_info()
        
        if collection_info["count"] > 0 and not force_rebuild:
            print(f"âœ… å‘é‡æ•°æ®åº“å·²å­˜åœ¨ï¼ŒåŒ…å« {collection_info['count']} ä¸ªæ–‡æ¡£å—")
            print("   å¦‚éœ€é‡å»ºï¼Œè¯·ä½¿ç”¨ force_rebuild=True")
        else:
            print("ğŸ“„ æ­£åœ¨å¤„ç†PDFæ–‡æ¡£...")
            self._build_vector_store()
        
        # 4. åˆå§‹åŒ–é—®ç­”é“¾
        print(f"\nğŸ¤– åˆå§‹åŒ–é—®ç­”é“¾: {self.llm_model}")
        self.qa_chain = BasicQAChain(
            retriever=self._create_retriever(),
            llm_model=self.llm_model
        )
        
        print("\nâœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 60)
    
    def _build_vector_store(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        # 1. åŠ è½½PDFæ–‡æ¡£
        print(f"   - åŠ è½½PDF: {self.pdf_path}")
        loader = PDFLoader(self.pdf_path)
        documents = loader.load()
        print(f"   - å…±åŠ è½½ {len(documents)} é¡µæ–‡æ¡£")
        
        # 2. åˆ†å‰²æ–‡æ¡£ï¼ˆä½¿ç”¨åŸºäºæ ‡é¢˜çš„åˆ†å‰²å™¨ï¼‰
        print(f"   - æŒ‰æ ‡é¢˜åˆ†å‰²æ–‡æ¡£ (max_chunk_size={self.max_chunk_size}, overlap={self.chunk_overlap})")
        splitter = HeadingBasedSplitter(
            max_chunk_size=self.max_chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        chunks = splitter.split(documents)
        print(f"   - å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # 3. ç”ŸæˆåµŒå…¥å‘é‡
        print(f"   - ç”ŸæˆåµŒå…¥å‘é‡...")
        texts = [doc.page_content for doc in chunks]
        embeddings = self.embedder.embed_documents(texts)
        print(f"   - åµŒå…¥å‘é‡ç»´åº¦: {len(embeddings[0])}")
        
        # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        print(f"   - å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        self.store.add_documents(chunks, embeddings)
        self.store.persist()
        print(f"   - æ•°æ®å·²æŒä¹…åŒ–åˆ°: {self.persist_directory}")
    
    def _create_retriever(self):
        """åˆ›å»ºæ£€ç´¢å™¨é€‚é…å™¨"""
        class VectorStoreRetriever:
            def __init__(self, store, embedder):
                self.store = store
                self.embedder = embedder
            
            def similarity_search(self, question: str, k: int = 4):
                """å°†é—®é¢˜è½¬æ¢ä¸ºåµŒå…¥å‘é‡ï¼Œç„¶åè¿›è¡Œç›¸ä¼¼åº¦æœç´¢"""
                query_embedding = self.embedder.embed_query(question)
                return self.store.similarity_search(query_embedding, k)
        
        return VectorStoreRetriever(self.store, self.embedder)
    
    def ask(self, question: str, k: int = 7) -> dict:
        """
        æé—®
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå‚è€ƒæ–‡æ¡£çš„å­—å…¸
        """
        if not self.qa_chain:
            raise RuntimeError("ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize() æ–¹æ³•")
        
        print(f"\nâ“ é—®é¢˜: {question}")
        print("-" * 60)
        
        result = self.qa_chain.run(question, k=k)
        
        print(f"ğŸ“ ç­”æ¡ˆ: {result['answer']}")
        
        if result['source_documents']:
            print(f"\nğŸ“š å‚è€ƒæ–‡æ¡£:")
            for i, doc in enumerate(result['source_documents'], 1):
                metadata = doc['metadata']
                page = metadata.get('page', 'æœªçŸ¥')
                similarity = doc.get('similarity_score', 0)
                print(f"   [{i}] é¡µç : {page} | ç›¸ä¼¼åº¦: {similarity:.3f}")
                # print(f"       å†…å®¹: {doc['content'][:100]}...")
        
        print("=" * 60)
        
        return result
    
    def interactive_mode(self):
        """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
        print("\nğŸ¯ è¿›å…¥äº¤äº’å¼é—®ç­”æ¨¡å¼")
        print("è¾“å…¥é—®é¢˜å¼€å§‹æé—®ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 60)
        
        while True:
            try:
                question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if not question:
                    continue
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break
                
                self.ask(question)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="å‘˜å·¥æ‰‹å†Œé—®ç­”ç³»ç»Ÿ")
    parser.add_argument(
        "--interactive", "-i",
        action="store_true",
        help="è¿›å…¥äº¤äº’å¼é—®ç­”æ¨¡å¼"
    )
    parser.add_argument(
        "--question", "-q",
        type=str,
        help="ç›´æ¥æé—®"
    )
    parser.add_argument(
        "--rebuild", "-r",
        action="store_true",
        help="å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“"
    )
    parser.add_argument(
        "--example", "-e",
        action="store_true",
        help="è¿è¡Œç¤ºä¾‹é—®é¢˜"
    )
    args = parser.parse_args()
    
    # åˆ›å»ºé—®ç­”ç³»ç»Ÿå®ä¾‹
    qa_system = EmployeeHandbookQA(
        pdf_path="data/å‘˜å·¥æ‰‹å†Œ.pdf",
        collection_name="employee_handbook",
        persist_directory="./chroma_db",
        max_chunk_size=1500,
        chunk_overlap=100,
        embedding_model="bge-m3:latest",
        llm_model="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
    )
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œforce_rebuild=False ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºå‘é‡æ•°æ®åº“
    # å¦‚æœéœ€è¦é‡å»ºï¼Œè®¾ç½® force_rebuild=True
    qa_system.initialize(force_rebuild=args.rebuild)
    
    # ç¤ºä¾‹é—®é¢˜
    example_questions = [
        "å‘˜å·¥çš„åˆä¼‘æ—¶é—´æ˜¯å‡ ç‚¹ï¼Ÿ",
        "å…¬å¸æœ‰å“ªäº›ç¦åˆ©å¾…é‡ï¼Ÿ",
        "è¯·å‡åˆ¶åº¦æ˜¯æ€æ ·çš„ï¼Ÿ",
        "å‘˜å·¥æ‰‹å†Œä¸­å…³äºåŠ ç­çš„è§„å®šæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒæ“ä½œ
    if args.question:
        # ç›´æ¥æé—®
        qa_system.ask(args.question)
    elif args.interactive:
        # äº¤äº’å¼é—®ç­”æ¨¡å¼
        print("\nğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
        for i, q in enumerate(example_questions, 1):
            print(f"   {i}. {q}")
        qa_system.interactive_mode()
    elif args.example:
        # è¿è¡Œç¤ºä¾‹é—®é¢˜
        print("\nğŸ’¡ è¿è¡Œç¤ºä¾‹é—®é¢˜:")
        for i, q in enumerate(example_questions, 1):
            print(f"\n[{i}] {q}")
            qa_system.ask(q)
    else:
        # é»˜è®¤ï¼šæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯å¹¶è¿è¡Œä¸€ä¸ªç¤ºä¾‹
        print("\nğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
        for i, q in enumerate(example_questions, 1):
            print(f"   {i}. {q}")
        
        print("\n" + "=" * 60)
        print("ğŸ“– ä½¿ç”¨è¯´æ˜:")
        print("   python src/main.py                    # è¿è¡Œç¤ºä¾‹é—®é¢˜")
        print("   python src/main.py -i                 # äº¤äº’å¼é—®ç­”æ¨¡å¼")
        print("   python src/main.py -q 'ä½ çš„é—®é¢˜'      # ç›´æ¥æé—®")
        print("   python src/main.py -e                 # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹é—®é¢˜")
        print("   python src/main.py -r                # å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“")
        print("=" * 60)
        
        # è¿è¡Œä¸€ä¸ªç¤ºä¾‹é—®é¢˜
        print("\nè¿è¡Œç¤ºä¾‹é—®é¢˜...")
        qa_system.ask(example_questions[0])


if __name__ == "__main__":
    main()
